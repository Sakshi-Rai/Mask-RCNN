{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(images,edgemaps,data_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Load VGG params from disk without FC layers A\n",
    "    Add branch layers (with deconv) after each CONV block\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    conv1_1 = conv_layer_vgg(images, \"conv1_1\")\n",
    "    conv1_2 = conv_layer_vgg(conv1_1, \"conv1_2\")\n",
    "    side_1 = side_layer(conv1_2, \"side_1\", 1)\n",
    "    pool1 = max_pool(conv1_2, 'pool1')\n",
    "\n",
    "    print('Added CONV-BLOCK-1+SIDE-1')\n",
    "\n",
    "    conv2_1 = conv_layer_vgg(pool1, \"conv2_1\")\n",
    "    conv2_2 = conv_layer_vgg(conv2_1, \"conv2_2\")\n",
    "    side_2 = side_layer(conv2_2, \"side_2\", 2)\n",
    "    pool2 = max_pool(conv2_2, 'pool2')\n",
    "\n",
    "    print('Added CONV-BLOCK-2+SIDE-2')\n",
    "\n",
    "    conv3_1 = conv_layer_vgg(pool2, \"conv3_1\")\n",
    "    conv3_2 = conv_layer_vgg(conv3_1, \"conv3_2\")\n",
    "    conv3_3 = conv_layer_vgg(conv3_2, \"conv3_3\")\n",
    "    side_3 = side_layer(conv3_3, \"side_3\", 4)\n",
    "    pool3 = max_pool(conv3_3, 'pool3')\n",
    "\n",
    "    print('Added CONV-BLOCK-3+SIDE-3')\n",
    "\n",
    "    conv4_1 = conv_layer_vgg(pool3, \"conv4_1\")\n",
    "    conv4_2 = conv_layer_vgg(conv4_1, \"conv4_2\")\n",
    "    conv4_3 = conv_layer_vgg(conv4_2, \"conv4_3\")\n",
    "    side_4 = side_layer(conv4_3, \"side_4\", 8)\n",
    "    pool4 = max_pool(conv4_3, 'pool4')\n",
    "\n",
    "    print('Added CONV-BLOCK-4+SIDE-4')\n",
    "\n",
    "    conv5_1 = conv_layer_vgg(pool4, \"conv5_1\")\n",
    "    conv5_2 = conv_layer_vgg(conv5_1, \"conv5_2\")\n",
    "    conv5_3 = conv_layer_vgg(conv5_2, \"conv5_3\")\n",
    "    side_5 = side_layer(conv5_3, \"side_5\", 16)\n",
    "\n",
    "    print('Added CONV-BLOCK-5+SIDE-5')\n",
    "\n",
    "    side_outputs = [side_1, side_2, side_3, side_4, side_5]\n",
    "\n",
    "    w_shape = [1, 1, len(side_outputs), 1]\n",
    "    fuse = conv_layer(tf.concat(side_outputs, axis=3),\n",
    "                                w_shape, name='fuse_1', use_bias=False,\n",
    "                                w_init=tf.constant_initializer(0.2))\n",
    "\n",
    "    print('Added FUSE layer')\n",
    "\n",
    "    # complete output maps from side layer and fuse layers\n",
    "    outputs = side_outputs + [fuse]\n",
    "\n",
    "    data_dict = None\n",
    "    print(\"Build model finished: {:.4f}s\".format(time.time() - start_time))\n",
    "\n",
    "def max_pool(bottom, name):\n",
    "    return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "def conv_layer_vgg(bottom, name):\n",
    "    \"\"\"\n",
    "        Adding a conv layer + weight parameters from a dict\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        filt = get_conv_filter(name)\n",
    "\n",
    "        conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        conv_biases = get_bias(name)\n",
    "        bias = tf.nn.bias_add(conv, conv_biases)\n",
    "\n",
    "        relu = tf.nn.relu(bias)\n",
    "        return relu\n",
    "\n",
    "def conv_layer(x, W_shape, b_shape=None, name=None,\n",
    "               padding='SAME', use_bias=True, w_init=None, b_init=None):\n",
    "\n",
    "    W = weight_variable(W_shape, w_init)\n",
    "    tf.summary.histogram('weights_{}'.format(name), W)\n",
    "\n",
    "    if use_bias:\n",
    "        b = bias_variable([b_shape], b_init)\n",
    "        tf.summary.histogram('biases_{}'.format(name), b)\n",
    "\n",
    "    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=padding)\n",
    "\n",
    "    return conv + b if use_bias else conv\n",
    "\n",
    "def deconv_layer(x, upscale, name, padding='SAME', w_init=None):\n",
    "\n",
    "    x_shape = tf.shape(x)\n",
    "    in_shape = x.shape.as_list()\n",
    "\n",
    "    w_shape = [upscale * 2, upscale * 2, in_shape[-1], 1]\n",
    "    strides = [1, upscale, upscale, 1]\n",
    "\n",
    "    W = weight_variable(w_shape, w_init)\n",
    "    tf.summary.histogram('weights_{}'.format(name), W)\n",
    "\n",
    "    out_shape = tf.stack([x_shape[0], x_shape[1], x_shape[2], w_shape[2]]) * tf.constant(strides, tf.int32)\n",
    "    deconv = tf.nn.conv2d_transpose(x, W, out_shape, strides=strides, padding=padding)\n",
    "\n",
    "    return deconv\n",
    "\n",
    "def side_layer(self, inputs, name, upscale):\n",
    "    \"\"\"\n",
    "        https://github.com/s9xie/hed/blob/9e74dd710773d8d8a469ad905c76f4a7fa08f945/examples/hed/train_val.prototxt#L122\n",
    "        1x1 conv followed with Deconvoltion layer to upscale the size of input image sans color\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "\n",
    "        in_shape = inputs.shape.as_list()\n",
    "        w_shape = [1, 1, in_shape[-1], 1]\n",
    "\n",
    "        classifier = conv_layer(inputs, w_shape, b_shape=1,\n",
    "                                     w_init=tf.constant_initializer(),\n",
    "                                     b_init=tf.constant_initializer(),\n",
    "                                     name=name + '_reduction')\n",
    "\n",
    "        classifier = deconv_layer(classifier, upscale=upscale,\n",
    "                                       name='{}_deconv_{}'.format(name, upscale),\n",
    "                                       w_init=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "        return classifier\n",
    "\n",
    "def get_conv_filter(name):\n",
    "    return tf.constant(data_dict[name][0], name=\"filter\")\n",
    "\n",
    "def get_bias(name):\n",
    "    return tf.constant(data_dict[name][1], name=\"biases\")\n",
    "\n",
    "def weight_variable(shape, initial):\n",
    "\n",
    "    init = initial(shape)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def bias_variable(shape, initial):\n",
    "\n",
    "    init = initial(shape)\n",
    "    return tf.Variable(init)\n",
    "\n",
    "def setup_testing(session):\n",
    "\n",
    "    \"\"\"\n",
    "        Apply sigmoid non-linearity to side layer ouputs + fuse layer outputs for predictions\n",
    "    \"\"\"\n",
    "\n",
    "    self.predictions = []\n",
    "\n",
    "    for idx, b in enumerate(self.outputs):\n",
    "        output = tf.nn.sigmoid(b, name='output_{}'.format(idx))\n",
    "        self.predictions.append(output)\n",
    "\n",
    "def setup_training(self, session):\n",
    "\n",
    "    \"\"\"\n",
    "        Apply sigmoid non-linearity to side layer ouputs + fuse layer outputs\n",
    "        Compute total loss := side_layer_loss + fuse_layer_loss\n",
    "        Compute predicted edge maps from fuse layer as pseudo performance metric to track\n",
    "    \"\"\"\n",
    "\n",
    "    self.predictions = []\n",
    "    self.loss = 0\n",
    "\n",
    "    self.io.print_warning('Deep supervision application set to {}'.format(self.cfgs['deep_supervision']))\n",
    "\n",
    "    for idx, b in enumerate(self.side_outputs):\n",
    "        output = tf.nn.sigmoid(b, name='output_{}'.format(idx))\n",
    "        cost = sigmoid_cross_entropy_balanced(b, self.edgemaps, name='cross_entropy{}'.format(idx))\n",
    "\n",
    "        self.predictions.append(output)\n",
    "        if self.cfgs['deep_supervision']:\n",
    "            self.loss += (self.cfgs['loss_weights'] * cost)\n",
    "\n",
    "    fuse_output = tf.nn.sigmoid(self.fuse, name='fuse')\n",
    "    fuse_cost = sigmoid_cross_entropy_balanced(self.fuse, self.edgemaps, name='cross_entropy_fuse')\n",
    "\n",
    "    self.predictions.append(fuse_output)\n",
    "    self.loss += (self.cfgs['loss_weights'] * fuse_cost)\n",
    "\n",
    "    pred = tf.cast(tf.greater(fuse_output, 0.5), tf.int32, name='predictions')\n",
    "    error = tf.cast(tf.not_equal(pred, tf.cast(self.edgemaps, tf.int32)), tf.float32)\n",
    "    self.error = tf.reduce_mean(error, name='pixel_error')\n",
    "\n",
    "    tf.summary.scalar('loss', self.loss)\n",
    "    tf.summary.scalar('error', self.error)\n",
    "\n",
    "    self.merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    self.train_writer = tf.summary.FileWriter(self.cfgs['save_dir'] + '/train', session.graph)\n",
    "    self.val_writer = tf.summary.FileWriter(self.cfgs['save_dir'] + '/val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
